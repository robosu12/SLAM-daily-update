import os
import re
import time
import base64
import requests
from datetime import datetime
from pathlib import Path

# ===================== é…ç½®å‚æ•° =====================
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")  # GitHub PAT
REPO_OWNER = "robosu12"       # GitHubç”¨æˆ·å
REPO_NAME = "SLAM-daily-update"              # ä»“åº“å
SEARCH_KEYWORDS = ["SLAM", "Simultaneous Localization and Mapping"]
TARGET_CONF = ["icra", "iros", "ral", "tro"]  # ç›®æ ‡ä¼šè®®/æœŸåˆŠ
PROCESSED_FILE = "processed_repos.txt"    # å·²å¤„ç†ä»“åº“è®°å½•
SKIP_EXISTED = True                       # å¯ç”¨å»é‡
ONLY_NEW_UPDATED = False                  # ä»…å¤„ç†è¿‘æœŸæ›´æ–°
TABLE_HEADER = """| æ ‡é¢˜ | ä½œè€… | ä¼šè®®/æœŸåˆŠ | å¹´ä»½ | ä»£ç ä»“åº“ | è®ºæ–‡é“¾æ¥ |
|------|------|-----------|------|----------|----------|"""
TABLE_TEMPLATE = "| {title} | {authors} | {conf} | {year} | [{repo}]({repo_url}) | [{paper}]({paper_url}) |"
# ===================================================

def github_api_request(url, params=None):
    """GitHub APIè¯·æ±‚ï¼ˆå¸¦å¼‚å¸¸å¤„ç†ï¼‰"""
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.HTTPError as e:
        if response.status_code == 403:
            reset_time = int(response.headers.get("X-RateLimit-Reset", time.time()))
            raise Exception(f"APIé€Ÿç‡é™åˆ¶ï¼Œé‡ç½®æ—¶é—´: {datetime.fromtimestamp(reset_time)}")
        elif response.status_code == 404:
            print(f"ä»“åº“ä¸å­˜åœ¨: {url}")
            return None
        else:
            print(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
            return None
    except Exception as e:
        print(f"è¯·æ±‚å¼‚å¸¸: {str(e)}")
        return None

def search_slam_repos():
    """æœç´¢ç¬¦åˆæ¡ä»¶çš„GitHubä»“åº“"""
    base_url = "https://api.github.com/search/repositories"
    query = (
        f"{' '.join(SEARCH_KEYWORDS)} "
        f"{' '.join(TARGET_CONF)} "
        "has:code in:description,topics "
        "-topic:documentation -topic:demo"
    )
    
    all_repos = []
    page = 1
    while True:
        params = {"q": query, "sort": "updated", "order": "desc", "per_page": 100, "page": page}
        data = github_api_request(base_url, params)
        if not data or not data.get("items"):
            break
            
        all_repos.extend(data["items"])
        print(f"å·²è·å–ç¬¬{page}é¡µï¼Œç´¯è®¡{len(all_repos)}ä¸ªä»“åº“")
        
        if len(data["items"]) < params["per_page"]:
            break
            
        page += 1
        time.sleep(1)  # éµå®ˆAPIé€Ÿç‡é™åˆ¶
    
    return all_repos

def load_processed_repos():
    """åŠ è½½å·²å¤„ç†ä»“åº“åˆ—è¡¨"""
    processed_path = Path(PROCESSED_FILE)
    if not processed_path.exists():
        return []
    
    try:
        with open(processed_path, "r", encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"åŠ è½½å·²å¤„ç†ä»“åº“å¤±è´¥: {str(e)}")
        return []

def save_processed_repos(repos):
    """ä¿å­˜å·²å¤„ç†ä»“åº“åˆ—è¡¨ï¼ˆå»é‡å­˜å‚¨ï¼‰"""
    try:
        unique_repos = sorted(set(repos))
        with open(PROCESSED_FILE, "w", encoding="utf-8") as f:
            f.write("\n".join(unique_repos))
        print(f"å·²ä¿å­˜{len(unique_repos)}ä¸ªå·²å¤„ç†ä»“åº“è®°å½•")
    except Exception as e:
        print(f"ä¿å­˜å·²å¤„ç†ä»“åº“å¤±è´¥: {str(e)}")

def parse_readme(readme_content, repo_info):
    """ä»READMEè§£æè®ºæ–‡å…ƒæ•°æ®"""
    patterns = {
        "title": r"## ğŸ“„ è®ºæ–‡æ ‡é¢˜\s*[:ï¼š]\s*([\s\S]+?)\s*(?=\n##|$)",
        "authors": r"## ğŸ‘¥ ä½œè€…\s*[:ï¼š]\s*([\s\S]+?)\s*(?=\n##|$)",
        "conf": r"## ğŸ“… ä¼šè®®/æœŸåˆŠ\s*[:ï¼š]\s*([\s\S]+?)\s*(?=\n##|$)",
        "year": r"## ğŸ“† å‘è¡¨å¹´ä»½\s*[:ï¼š]\s*(\d{4})\s*(?=\n##|$)",
        "paper": r"## ğŸ“œ è®ºæ–‡é“¾æ¥\s*[:ï¼š]\s*([\s\S]+?)\s*(?=\n##|$)"
    }
    
    result = {}
    for key, pattern in patterns.items():
        try:
            match = re.search(pattern, readme_content, re.IGNORECASE)
            result[key] = match.group(1).strip() if match else "æœªæä¾›"
        except Exception as e:
            print(f"è§£æ{key}å¤±è´¥: {str(e)}")
            result[key] = "è§£æé”™è¯¯"
    
    # è‡ªåŠ¨æ¨æ–­ä¼šè®®ç±»å‹
    if result["conf"] in ["æœªæä¾›", "è§£æé”™è¯¯"]:
        desc_lower = (repo_info.get("description") or "").lower()
        for conf in TARGET_CONF:
            if conf in desc_lower:
                result["conf"] = conf.upper()
                break
        else:
            result["conf"] = "å…¶ä»–ä¼šè®®"
    
    return result

def update_readme_table(rows):
    """æ›´æ–°READMEä¸­çš„è®ºæ–‡è¡¨æ ¼"""
    try:
        readme_path = Path("README.md")
        content = ""
        
        if readme_path.exists():
            with open(readme_path, "r", encoding="utf-8") as f:
                content = f.read()
        
        # å®šä½è¡¨æ ¼ä½ç½®
        start_idx = content.find(TABLE_HEADER)
        if start_idx == -1:
            # æ·»åŠ æ–°è¡¨æ ¼
            content = f"# SLAMå¼€æºè®ºæ–‡åˆé›†\n\n## æœ€æ–°å¼€æºè®ºæ–‡\n{TABLE_HEADER}\n" + "\n".join(rows)
        else:
            # æ›´æ–°ç°æœ‰è¡¨æ ¼
            end_idx = content.find("\n## ", start_idx)
            if end_idx == -1:
                end_idx = len(content)
            content = content[:start_idx] + TABLE_HEADER + "\n" + "\n".join(rows) + content[end_idx:]
        
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        print(f"READMEæ›´æ–°å®Œæˆï¼Œæ·»åŠ {len(rows)}ç¯‡è®ºæ–‡")
        return True
    except Exception as e:
        print(f"æ›´æ–°READMEå¤±è´¥: {str(e)}")
        return False

def main():
    # 1. åŠ è½½å·²å¤„ç†ä»“åº“
    processed_repos = load_processed_repos()
    print(f"å·²åŠ è½½{len(processed_repos)}ä¸ªå·²å¤„ç†ä»“åº“")
    
    # 2. æœç´¢æ–°ä»“åº“
    print("æœç´¢ç¬¦åˆæ¡ä»¶çš„æ–°ä»“åº“...")
    all_repos = search_slam_repos()
    if not all_repos:
        print("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä»“åº“")
        return
    
    # 3. è¿‡æ»¤ä»“åº“ï¼ˆå»é‡+æ—¶é—´è¿‡æ»¤ï¼‰
    new_repos = []
    for repo in all_repos:
        repo_fullname = repo["full_name"]
        
        # è·³è¿‡å·²å¤„ç†çš„ä»“åº“
        if SKIP_EXISTED and repo_fullname in processed_repos:
            continue
            
        # æ—¶é—´è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        if ONLY_NEW_UPDATED:
            updated_at = datetime.strptime(repo["updated_at"], "%Y-%m-%dT%H:%M:%SZ")
            if (datetime.now() - updated_at).days > 7:
                continue
                
        new_repos.append(repo)
    
    if not new_repos:
        print("æ— æ–°ä»“åº“éœ€è¦å¤„ç†")
        return
    print(f"å‘ç°{len(new_repos)}ä¸ªæ–°ä»“åº“å¾…å¤„ç†")
    
    # 4. å¤„ç†æ–°ä»“åº“
    table_rows = []
    processed_in_this_run = []
    for repo in new_repos[:50]:  # é™åˆ¶50ä¸ªé˜²æ­¢è¶…æ—¶
        repo_fullname = repo["full_name"]
        repo_url = repo["html_url"]
        
        try:
            # è·å–ä»“åº“è¯¦æƒ…
            repo_detail = github_api_request(f"https://api.github.com/repos/{repo_fullname}")
            if not repo_detail:
                print(f"è·³è¿‡æ— æ³•è·å–è¯¦æƒ…çš„ä»“åº“: {repo_fullname}")
                continue
                
            # è·å–READMEå†…å®¹
            readme_content = ""
            default_branch = repo_detail.get("default_branch", "main")
            readme_resp = github_api_request(
                f"https://api.github.com/repos/{repo_fullname}/contents/README.md",
                {"ref": default_branch}
            )
            
            if readme_resp and "content" in readme_resp:
                readme_content = base64.b64decode(readme_resp["content"]).decode("utf-8")
            
            # è§£æè®ºæ–‡ä¿¡æ¯
            paper_info = parse_readme(readme_content, repo)
            
            # ç”Ÿæˆè¡¨æ ¼è¡Œ
            table_row = TABLE_TEMPLATE.format(
                title=paper_info["title"],
                authors=paper_info["authors"],
                conf=paper_info["conf"],
                year=paper_info["year"],
                repo=f"{repo_fullname}",
                repo_url=repo_url,
                paper=paper_info["paper"],
                paper_url=paper_info["paper"]
            )
            table_rows.append(table_row)
            processed_in_this_run.append(repo_fullname)
            print(f"æˆåŠŸå¤„ç†: {repo_fullname} ({paper_info['conf']} {paper_info['year']})")
            
            time.sleep(0.5)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡
            
        except Exception as e:
            print(f"å¤„ç†ä»“åº“ {repo_fullname} æ—¶å‡ºé”™: {str(e)}")
    
    # 5. æ›´æ–°READMEå’Œè®°å½•
    if table_rows:
        # æŒ‰å¹´ä»½é™åºæ’åº
        table_rows.sort(key=lambda x: int(re.search(r"\d{4}", x).group()), reverse=True)
        
        if update_readme_table(table_rows):
            # æ›´æ–°å·²å¤„ç†è®°å½•
            new_processed = processed_repos + processed_in_this_run
            save_processed_repos(new_processed)
            print(f"å·²ä¿å­˜{len(processed_in_this_run)}ä¸ªæ–°å¤„ç†è®°å½•")

if __name__ == "__main__":
    main()
